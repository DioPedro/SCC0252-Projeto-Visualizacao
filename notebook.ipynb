{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualização de Dados Covid 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alunos\n",
    "Alexandre Brito Gomes - 11857323\n",
    "\n",
    "Diógenes Silva Pedro - 11883476\n",
    "\n",
    "Gabriel Freitas Ximenes Vasconcelos - 11819084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta visualização, analisaremos o conjunto de dados \"Coronavirus - Brazil\", disponível no Kaggle em https://www.kaggle.com/datasets/unanimad/corona-virus-brazil?select=brazil_covid19_cities.csv . Nele, é possível obter informações sobre o número de casos de Coronavirus no Brasil de maneira temporal: apresentando o número de casos, de maneira cumulativa, desde 30 de janeiro de 2020, quando o primeiro caso suspeito foi encontrado no país. \n",
    "\n",
    "Esse conjunto de dados é composto por 6 arquivos csv, contendo dados como latitude e longitude das cidades, data da confirmação de primeiro caso, número de casos confirmados, número de mortes, recuperados, suspeitos, entre diversos outros.\n",
    "\n",
    "Com isso, é possível fazer análises como verificação de quais localidades tiveram maiores números de casos em pouco tempo, como foi a evolução dos casos ao longo do tempo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import json\n",
    "from urllib.request import urlopen\n",
    "import math\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importação dos datasets base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'covid-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigla_estado = json.load(open(\"sigla_estado.json\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigla(row):\n",
    "    return sigla_estado[row['state']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population = pd.read_csv(data_path + 'brazil_population_2019.csv')\n",
    "df_population[\"state_sigla\"] = df_population.apply(get_sigla, axis=1)\n",
    "df_population.rename({'state': 'full_state'}, inplace=True, axis=1)\n",
    "df_population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities = pd.read_csv(data_path + \"brazil_covid19_cities.csv\")\n",
    "df_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = df_population.drop_duplicates(['full_state'])\n",
    "area_dict = dict(zip(states.state_code, states.state_sigla))\n",
    "print(area_dict)\n",
    "def get_sigla(row):\n",
    "    return area_dict[row['state_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coordinates = pd.read_csv(data_path + \"brazil_cities_coordinates.csv\")\n",
    "df_coordinates['state_sigla'] = df_coordinates.apply(get_sigla, axis=1)\n",
    "df_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do df_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_cities.merge(df_population[['city', 'population', 'region', 'full_state', 'state_sigla']], how='inner', left_on = ['name', 'state'], right_on=['city', 'state_sigla'])\n",
    "df_city = df_city.merge(df_coordinates[['city_name', 'city_code', 'lat', 'long', 'state_code', 'state_sigla']], how='inner', left_on=['name', 'state'], right_on=['city_name', 'state_sigla'])\n",
    "df_city = df_city.drop([\"code\", \"name\", \"city_name\", \"state_sigla_x\", \"state_sigla_y\"], axis='columns')\n",
    "df_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.sort_values(['city', 'state', 'date'], inplace=True)\n",
    "df_city.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolver o problema da acumulação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As colunas `death` e `mortes` são colunas que idealmente deveriam ser acumuladas. No entanto, ao analisar o dataset podemos ver que há casos em que valores diminuem.\n",
    "O dataset possui 2770 municípios e apenas 266 não possuem esse problema, por isso, devemos dar a devida atenção a esse problema nos dados.\n",
    "\n",
    "Para amenizar isso, criamos a função `fix_accumulation` que funciona da seguinte maneira:\n",
    "\n",
    "1. Defino como `start` a posição cuja a posição seguinte é um valor menor\n",
    "2. Percorro o array até achar uma posição cujo valor é igual ou superior ao valor do `start`. Essa posição é a `end`\n",
    "3. Vejo a quantidade de elementos entre `start` e `end` e faço a seguinte equação que representa o coeficiente angular da reta:\n",
    "   $$\n",
    "   \\Delta y = \\frac{\\text{arr[end]} - \\text{arr[start]}}{\\text{end}-\\text{start}}\n",
    "   $$\n",
    "4. Incremento $\\Delta y$ a partir de cada valor, começando de `start` até chegar ao `end`\n",
    "5. Caso um `start` comece e o array termine antes de chegar ao `end`, os valores seguintes ao `start` são substituídos por ele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_accumulation(original_arr):\n",
    "    arr = original_arr.copy()\n",
    "    prev = arr[0]\n",
    "    start = 0\n",
    "    end = 0\n",
    "    interval = False\n",
    "    for i, el in enumerate(arr):\n",
    "        if el < prev and interval == False:\n",
    "            start = i-1\n",
    "            interval = True\n",
    "            \n",
    "        if el >= arr[start] and interval == True:\n",
    "            end = i\n",
    "            interval=False\n",
    "            delta = (arr[end]-arr[start])/(end-start)\n",
    "            for j in range(start+1, end):\n",
    "                arr[j] = int(math.floor(arr[j-1] + delta))\n",
    "                \n",
    "        prev = arr[i]\n",
    "        \n",
    "    if interval == True:\n",
    "        for i in range(start+1, len(arr)):\n",
    "            arr[i] = arr[start]\n",
    "            \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_column(coluna):\n",
    "    problemas_city = list(df_city.loc[df_city[coluna + ' per day'] < 0].drop_duplicates(subset=['city', 'state'], inplace=False)[\"city\"])\n",
    "    problemas_state = list(df_city.loc[df_city[coluna + ' per day'] < 0].drop_duplicates(subset=['city', 'state'], inplace=False)[\"state\"])\n",
    "    print(f\"Cidades com o problema na coluna {coluna}: {len(problemas_city)}\")\n",
    "\n",
    "    indexes = []\n",
    "    for i in range(len(problemas_city)):\n",
    "        citystate_index = df_city.loc[(df_city['city'] == problemas_city[i]) & (df_city['state'] == problemas_state[i])].index\n",
    "        indexes.append(citystate_index)\n",
    "\n",
    "    lists_of_cases = [df_city.iloc[city][coluna].to_list() for city in indexes]\n",
    "    fixed_cases = [fix_accumulation(city) for city in lists_of_cases]\n",
    "\n",
    "    for i in range(len(fixed_cases)):\n",
    "        city_indexes = indexes[i]\n",
    "        fixed_city = fixed_cases[i]\n",
    "        df_city.loc[city_indexes, coluna]=fixed_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos as colunas `cases per day` e `mortes per day` para sabermos onde isso dá problema\n",
    "\n",
    "cum_columns = ['cases', 'deaths']\n",
    "df_city = df_city.merge(\n",
    "    df_city.groupby(['city', 'state'])[cum_columns].diff(),\n",
    "    left_index=True, right_index=True, suffixes=['', ' per day']\n",
    ").fillna({'{} per day'.format(cum_column): df_city[cum_column] for cum_column in cum_columns})\n",
    "\n",
    "df_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_column(\"cases\")\n",
    "solve_column(\"deaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropo as colunas antigas incorretas\n",
    "df_city = df_city.drop(columns=['cases per day', 'deaths per day'])\n",
    "\n",
    "# recrio as colunas só que com a agregação correta\n",
    "cum_columns = ['cases', 'deaths']\n",
    "df_city = df_city.merge(\n",
    "    df_city.groupby(['city', 'state'])[cum_columns].diff(),\n",
    "    left_index=True, right_index=True, suffixes=['', ' per day']\n",
    ").fillna({'{} per day'.format(cum_column): df_city[cum_column] for cum_column in cum_columns})\n",
    "\n",
    "df_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifico se não há nenhuma coluna com problema de acumulação após o processamento\n",
    "quant_cases = df_city.loc[df_city['cases per day'] < 0].drop_duplicates(subset=['city', 'state']).shape[0]\n",
    "quant_deaths = df_city.loc[df_city['deaths per day'] < 0].drop_duplicates(subset=['city', 'state']).shape[0]\n",
    "print(f\"Cidades com o problema na coluna cases após correção: {quant_cases}\")\n",
    "print(f\"Cidades com o problema na coluna deaths após correção: {quant_deaths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenho uma normalização por meio do tamanho da população da cidade\n",
    "df_city[\"cases/population\"] = df_city[\"cases\"]/df_city[\"population\"]\n",
    "df_city[\"deaths/population\"] = df_city[\"deaths\"]/df_city[\"population\"]\n",
    "df_city[\"cases per day/population\"] = df_city[\"cases per day\"]/df_city[\"population\"]\n",
    "df_city[\"deaths per day/population\"] = df_city[\"deaths per day\"]/df_city[\"population\"]\n",
    "\n",
    "df_city.rename(columns={'long': 'lon'}, inplace=True)\n",
    "\n",
    "df_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.to_csv('etl_data/city.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do df_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenho o número de casos e mortes por dia por estado\n",
    "df_state = df_city.groupby(['state', 'date'])[['cases per day', 'deaths per day']].sum()\n",
    "df_state.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adiciono a população do estado, somando as populações dos municípios do estado\n",
    "state_population = df_city.drop_duplicates(subset=['city', 'state']).groupby('state')['population'].sum()\n",
    "population_dict = state_population.to_dict()\n",
    "\n",
    "def add_population(row):\n",
    "    return population_dict[row['state']]\n",
    "\n",
    "df_state['population'] = df_state.apply(add_population, axis=1)\n",
    "df_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculo uma média da latitude e longitude dos municípios do estado\n",
    "state_avg_coord = df_city.drop_duplicates(subset=['city', 'state']).groupby('state')[['lat', 'lon']].mean()\n",
    "\n",
    "# adiciono a lat e long média, além de adicionar a região do estado\n",
    "df_state = df_state.merge(state_avg_coord, on='state').merge(df_city[['state', 'region']].drop_duplicates(subset=['state']), on='state')\n",
    "\n",
    "# calculo e adiciono o número de casos e mortes acumulados do estado\n",
    "df_state['cases'] = df_state.groupby('state')['cases per day'].cumsum(axis=0)\n",
    "df_state['deaths'] = df_state.groupby('state')['deaths per day'].cumsum(axis=0)\n",
    "\n",
    "# Obtenho uma normalização por meio do tamanho da população do estado\n",
    "df_state[\"cases/population\"] = df_state[\"cases\"]/df_state[\"population\"]\n",
    "df_state[\"deaths/population\"] = df_state[\"deaths\"]/df_state[\"population\"]\n",
    "df_state[\"cases per day/population\"] = df_state[\"cases per day\"]/df_state[\"population\"]\n",
    "df_state[\"deaths per day/population\"] = df_state[\"deaths per day\"]/df_state[\"population\"]\n",
    "df_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state.to_csv('etl_data/state.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do df_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenho o número de casos e mortes por dia por estado\n",
    "df_region = df_state.groupby(['region', 'date'])[['cases per day', 'deaths per day']].sum()\n",
    "df_region.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adiciono a população do estado, somando as populações dos municípios do estado\n",
    "region_population = df_state.drop_duplicates(subset=['state']).groupby('region')['population'].sum()\n",
    "population_dict = region_population.to_dict()\n",
    "\n",
    "def add_population(row):\n",
    "    return population_dict[row['region']]\n",
    "\n",
    "df_region['population'] = df_region.apply(add_population, axis=1)\n",
    "df_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculo uma média da latitude e longitude dos municípios do estado\n",
    "region_avg_coord = df_state.drop_duplicates(subset=['state']).groupby('region')[['lat', 'lon']].mean()\n",
    "\n",
    "# adiciono a lat e long média, além de adicionar a região do estado\n",
    "df_region = df_region.merge(region_avg_coord, on='region')\n",
    "\n",
    "# calculo e adiciono o número de casos e mortes acumulados do estado\n",
    "df_region['cases'] = df_region.groupby('region')['cases per day'].cumsum(axis=0)\n",
    "df_region['deaths'] = df_region.groupby('region')['deaths per day'].cumsum(axis=0)\n",
    "\n",
    "# Obtenho uma normalização por meio do tamanho da população do estado\n",
    "df_region[\"cases/population\"] = df_region[\"cases\"]/df_region[\"population\"]\n",
    "df_region[\"deaths/population\"] = df_region[\"deaths\"]/df_region[\"population\"]\n",
    "df_region[\"cases per day/population\"] = df_region[\"cases per day\"]/df_region[\"population\"]\n",
    "df_region[\"deaths per day/population\"] = df_region[\"deaths per day\"]/df_region[\"population\"]\n",
    "df_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region.to_csv('etl_data/region.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar CSVs prontos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = pd.read_csv(\"etl_data/city.csv\", index_col=0)\n",
    "df_state = pd.read_csv(\"etl_data/state.csv\", index_col=0)\n",
    "df_region = pd.read_csv(\"etl_data/region.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação do gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cities_coordinates = pd.read_csv(data_path + \"brazil_cities_coordinates.csv\")\n",
    "df_cities_coordinates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_region.drop_duplicates('region')[[\"lat\", \"lon\"]].mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brazil_code = 100\n",
    "with urlopen(f'https://raw.githubusercontent.com/tbrugz/geodata-br/master/geojson/geojs-{brazil_code}-mun.json') as response:\n",
    "    brazil_json = json.loads(response.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.loc[(df_city['city'] == 'São Carlos') & (df_city['state'] == 'SP')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.iloc[1024081]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city.loc[(df_city['city'] == 'São Carlos')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth_mapbox(df_city.loc[df_city['date'] == '2021-03-29'], geojson=brazil_json, featureidkey = 'properties.id', locations='city_code', color='cases/population',\n",
    "                           color_continuous_scale=\"Viridis\",\n",
    "                           mapbox_style=\"carto-positron\",\n",
    "                           zoom=3, center = df_region.drop_duplicates('region')[[\"lat\", \"lon\"]].mean().to_dict(),\n",
    "                           opacity=0.5\n",
    "                          )\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
